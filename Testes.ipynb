{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import pandas as pd\n",
    "import Convert_NRC as nrc\n",
    "from header import EmotionAnalysis as ea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nrc.create_emolex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath = 'books/senhora.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw = ea.open_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = ea.tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered = ea.filter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = ea.convert_to_text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dist = ea.tokens_frequency(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ea.info(tokens, filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq = ea.most_frequent(filtered, dist, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"20 palavras mais frequentes: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Palavra mais frequente: {dist.max()} - {dist[dist.max()]} vezes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Colocações significantes: \\n\")\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ea.context(text, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordList, emotionList = ea.emolex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emoList = ea.newList(emotionList, filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emotionCounts = ea.generate_count(wordList, filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emotionCounts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordCounts = ea.generate_count(emoList, filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordCounts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for w in wordCounts.most_common(20):\n",
    "    print(w[0])\n",
    "    print(wordList[w[0]])\n",
    "    print('------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import OrderedDict, defaultdict, Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text.concordance(\"casamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text.similar(\"casamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text.common_contexts([freq[0][0], freq[1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sortedToken = sorted(list(set(filtered)), key=lambda token: dist[token], reverse=True)\n",
    "text.dispersion_plot(sortedToken[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordArray = np.asarray(wordCounts.most_common(5))\n",
    "wordArray = [i[0] for i in wordArray]\n",
    "text.dispersion_plot(wordArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratar negação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def filter(tokens):\n",
    "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "    stopwords = [s for s in stopwords if s != 'não']\n",
    "    \n",
    "    \n",
    "    filtered = [t for t in tokens \n",
    "                if t not in stopwords \n",
    "                and t.isalpha() \n",
    "                and len(t) > 1]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = filter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_count():\n",
    "    emoCount = Counter()\n",
    "    t = singular\n",
    "    for i in range(len(singular) - 1):\n",
    "        if len(wordList[t[i]]) > 0:\n",
    "            if t[i-1] == 'não':\n",
    "                wordList[t[i]] = revert_emotion(wordList[t[i]])\n",
    "            emoCount += Counter(wordList[t[i]])\n",
    "    return emoCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_emotion(wordList):\n",
    "    newList = []\n",
    "    for w in wordList:\n",
    "        if w == 'positivo':\n",
    "            newList.append('negativo')\n",
    "        elif w == 'negativo':\n",
    "            newList.append('positivo')\n",
    "        elif w == 'alegria':\n",
    "            newList.append('tristeza')\n",
    "        elif w == 'tristeza':\n",
    "            newList.append('alegria')\n",
    "        elif w == 'antecipação':\n",
    "            newList.append('surpresa')\n",
    "        elif w == 'surpresa':\n",
    "            newList.append('antecipação')\n",
    "        elif w == 'medo':\n",
    "            newList.append('raiva')\n",
    "        elif w == 'raiva':\n",
    "            newList.append('medo')\n",
    "        elif w == 'nojo':\n",
    "            newList.append('confiança')\n",
    "        elif w == 'confiança':\n",
    "            newList.append('nojo')\n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newCounts = generate_count()\n",
    "newCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(new_tokens) - 1):\n",
    "    if new_tokens[i] == 'não':\n",
    "        new_tokens[i:i+2] = [' '.join(new_tokens[i:i+2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratar plural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "s = re.compile('s$')\n",
    "oes = re.compile('ões$|ãos$|ães$')\n",
    "res = re.compile('res$')\n",
    "zes = re.compile('zes$')\n",
    "ses = re.compile('ses$')\n",
    "ais = re.compile('ais$')\n",
    "eis = re.compile('éis$')\n",
    "ois = re.compile('óis$')\n",
    "uis = re.compile('uis$')\n",
    "eis = re.compile('is$|eis$')\n",
    "ns = re.compile('ns$')\n",
    "nes = re.compile('nes$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def singularize(tokens):\n",
    "    new_t = []\n",
    "    \n",
    "    for t in tokens:\n",
    "        if t.endswith('ões') | t.endswith('ãos') | t.endswith('ães') :\n",
    "            new_t.append(re.sub(oes, 'ão', t))\n",
    "        elif t.endswith('res'):\n",
    "            new_t.append(re.sub(res, 'r', t))\n",
    "        elif t.endswith('zes'):\n",
    "            new_t.append(re.sub(zes, 'z', t))\n",
    "        elif t.endswith('ses'):\n",
    "            new_t.append(re.sub(ses, 's', t))\n",
    "        elif t.endswith('ais'):\n",
    "            new_t.append(re.sub(ais, 'al', t))\n",
    "        elif t.endswith('éis'):\n",
    "            new_t.append(re.sub(eis, 'el', t))\n",
    "        elif t.endswith('óis'):\n",
    "            new_t.append(re.sub(ois, 'ol', t))\n",
    "        elif t.endswith('uis'):\n",
    "            new_t.append(re.sub(uis, 'ul', t))\n",
    "        elif t.endswith('is') | t.endswith('eis') :\n",
    "            new_t.append(re.sub(eis, 'il', t))\n",
    "        elif t.endswith('ns'):\n",
    "            new_t.append(re.sub(ns, 'm', t))\n",
    "        elif t.endswith('nes'):\n",
    "            new_t.append(re.sub(nes, 'n', t))\n",
    "        elif t.endswith('s'):\n",
    "            new_t.append(re.sub(s, '', t))\n",
    "        else:\n",
    "            new_t.append(t)\n",
    "            \n",
    "    return new_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "singular = singularize(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratar gênero dos adjetivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratar verbos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
